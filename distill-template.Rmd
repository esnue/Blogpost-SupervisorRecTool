---
title: "Thesis Supervisor Recommendation Tool"
description: "Using the Contextualized Topic Model (CTM), we identify academic topics covered by the faculty of the Hertie School in their published papers."
author: 
  - name: Lena Wagner 
    url: 
  - name: Ba Linh Le 
date: "`r Sys.Date()`" 
categories: 
  - Natural Language Processing 
creative_commons: CC BY
repository_url: https://github.com/esnue/ThesisAllocationSystem
output: 
  distill::distill_article: 
    self_contained: false
preview: figures/CTM.png
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
library(reticulate) # For rendering Python code 
```

## Abstract 

Using the Contextualized Topic Model (CTM), we identify academic topics covered by the faculty of the Hertie School in their published papers. The outputs of the model will be available on a web application that aims to support students in their thesis supervisor selection. On the web application, students can get an accessible overview of all potential supervisors' research fields to find the one that can best guide them substantively. The tool is especially recommended for students that have not been able to get familiar with the whole body of the faculty and would like to make sure their supervisor is the best fit for them. The CTM is most successful in identifying coherent topics on texts that entails proper sentences with a semantic structure as opposed to bullet points with very low word count. 
 

## Introduction / Background

If you've happened to come across this website, you're either a student that is taking the NLP class at Hertie and would like to get a great example of a potential semester project or you're a 2nd year student anxious to get started on the MA thesis road. In any case, you've come to the right place.\n

This project was inspired by the recurring challenge in the fall semester, when Hertie students and administrative staff alike have to match suitable and available supervisors to students' master theses. The task is - as you can probably imagine - very laborious. As a Hertie student, you are required to indicate three preferences of supervising professors in decreasing order of rank. But what if you have no idea? You've probably only met 5 professors, but there are 28 options out there! The agony. Worst of all? There is no good overview on the Moodle platform: you're forced to manually and individually download the supervision plan. 

```{r fig1, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: Own work."}
knitr::include_graphics("figures/meme1.jpg")
```

But don't lose hope yet because your seniors are here to come to your aid! We know that the choice of the supervisor matters. It certainly is relevant to the progress and quality of your thesis, potentially even to your work opportunities after graduation! That is why we have developed this tool. \n

The tool provides a lucid and accessible overview over the wide range of fields of academic expertise in the school's faculty body. It expands the work of last semester's student from the class "Python for Data Scientists", who built a simple LDA model and similarity index based on user input.^[Available under (https://github.com/cbsobral/python)] Sounds like a mouthful? Is a mouthful. No worries, everything is going to be explained below. Just stay with us!\n 

However, in case you are familar with these terms and totally know what's going on, here is the **TDLR**: LDA's biggest advantage, Bag-of-Words, turns out to be its deficiency. CTM addresses the lack of contextual information capture by integrating pre-trained BERT sentence embeddings. [@bianchi2020pre] CTM is a fairly new model, but it's proven to be super successful in our case!


## Related Work 

We've been throwing around some fancy terms making us appear real clever, but that is not the goal of this blog post. We want you to understand what's going on! After all, what is a teacher's worth if the students don't get it?\n

The general challenge of topic modeling is the detection of underlying topics in documents and the correct identification of patterns to match documents to topics. Documents describes the text unit of interest such as individual Twitter tweets, mails, papers, reports, etc. So the goal is to be able to identify topics and to create a pattern that is able to match unseen documents to established topics.\n

Let's say you've been tasked with sorting mail. Super tedious work. As you begin to go through each of the letters, you identify categories of letters: bills, advertisement, reminders and notifications of changes. The first pile is done, now on to the next pile we go. However, this pile is somewhat different from the first pile. However, you're clever and confident that you can clearly match the unseen letters to the categories (topics) you've established in the first pile. Get it?\n 

But how does a machine even comprehend text? Easy - by numbers! Every word is turned into a token, i.e. a number. We call that step "encoding". Fancy term there! Now that we've established the foundations, let's get into the nitty-gritty details!\n

```{r fig2, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971"}
knitr::include_graphics("figures/bow.png")
```

Latent Dirichlet Allocation (LDA) is a popular machine learning probabilistic algorithm used for topic modeling. The figure below illustrates how it basically works. It discovers topics and then matches these topics to documents. In the end, you get a table of documents with their proportional share to each topics. Going back to our mailbox example: let's say the first letter (document) is advertisement but there is also a bit of a reminder. Ideally, LDA would return that the document of interest is 60\% advertisement and 39\% reminder, 0.5\% bill and 0.5\% notification of changes. Why 0.5\%? That is how probability works! You can never fully rule out that something happens, you just know that it is veeeeery unlikely to happen.\n

```{r fig3, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html"}
knitr::include_graphics("figures/lda.png")
```

But what's machine learning? Standard LDA models are trained once on a training set, i.e. there is one process of identifying and establishing topics. It might be a long process but after the process is done, there is no more training. The model is left untouched, its sole purpose then is to be employed. That is different for deep learning models! They're configured to continuously learn. Every time they get new input, parameters inside the models are set to achieve optimal performance.\n 

But how exactly does LDA find topics? The answer to that very good question is Bag-of-Words (BoW - not to be mistaken with the immensely popular Nintendo Switch game Breath of the Wild (BOTW)). Each document is considered a collection of words, the goal is to identify words distinctive to the document. What then? We get the Term Frequency to get the raw count of words within a document and Inverse Document Frequency to indicate the informative value of words by their frequency in a document compared to their frequency across all other documents. So Term Frequency is interested in what words are used very often and Inverse Document Frequency acts a checks and balance! Words such as "and", "or", "but", etc. appear very often but they do not carry any informative value.

The challenge of topic modeling usually lies in the unsupervised detection of underlying topics in documents and the correct identification of patterns to match documents to topics. As a machine learning technique, Latent Dirichlet Allocation (LDA) is a popular probabilistic algorithm used for topic modeling. The Toronto Paper Matching System is also based on the LDA machine learning model. The Toronto Paper Matching System matches papers to initial expertise scores based on reviewers’ publications and cross-checks these matched with elicited scores based on reviewers’ self-assessed expertise.  Operating on a Bag-of-Words (BoW) representation, the Toronto Paper Matching System is able to do unsupervised classification of documents [@charlin2013toronto]. However, if each document is considered a collection of words, the goal is to identify indicative words for the document. The structure of the document itself, i.e. the grammatical role and the order of words, is not relevant to this model. Because the goal is to identify words distinctive to the document, certain words that do not carry any meaning for that purpose are removed before running the model. We refer to such words as "stop words".  Hence, the main limitation of LDA is that is unable to comprehend the semantics and context of words.\n

The word2vec approach attempts to overcome that particular deficiency by the use of word embeddings. Word embeddings generally constitute learned representation for text with real-number vectors where words try to capture as much of the semantic, lexical and context information as possible. word2vec combines two different learning models, Continuous Bag of Words (CBoW), predicting a word given its context, and Skip-Gram, predicting the context given a word. Thus, using neural networks, the vectors are created by predicting for each word potential neighboring words. Words that have the similar meanings end up with similar representations.Though word2vec attempts to integrate the context, it only provides one vector numeric representation for each word. For example, the word "bank" would erroneously have the same numeric representation in the sentences *"We went to the river **bank**."*and *"I need to go to the **bank** to make a deposit."*. In combining different meanings and usages of the same word into a single vector, word2vec inadvertently ends up generating context-independent word embeddings.\n

In contrast to word2vec, the Pre-trained language model (PLM) Bidirectional Encoder Representations from Transformers (BERT) generates word embeddings that facilitate multiple vector numeric representations for the same word, depending on the context of the word in any given sentence. Thus, BERT word embeddings are context-dependent. Moreover, the PLM BERT exceeds the performance of uni- or single-direction language models by applying bidirectional training of the attention model Transformer to language modeling.\n 

Lastly, in previous stages of our work, we have also contemplated the employment of BERTopic , which leverages BERT embeddings and c-TF-IDF to create easily interpretable topics [@grootendorst2020bertopic]. TF-IDF combines two methods to generate features from textual documents: Term Frequency (FT) to get the raw count of words within a document and Inverse Document Frequency (IDF) to indicate the informative value of words by their frequency in a document compared to their frequency across all other documents. Thus, common words such as "and" would be punished by IDF for being of no value to the distinction of documents and the identification of topics. The transformation of TF-IDF to c-TF-IDF then involves the merging of all documents in one long document to concentrate on classes, i.e. topics. While promising at first, the performance of BERTopic was soon outrun by the other models, *ceteris paribus*.  


## Proposed Method 

CTM is based on the neural topic model Neural-ProdLDA and a contextualized document embedded representation created by Sentence-BERT (SBERT). Neural-ProdLDA trains a neural inference network to transform the Bag of Words (BoW) document representation of a given document into a continuous latent representation via the two parameters $\mu$ and $\sigma^2$ of a Gaussian distribution. Instead of using a multinomial distribution over individual words as is common in standard LDA, Neural-ProdLDA uses the weighted product of experts, leading to stark improvements in topic coherence. The continuous latent representation is then decoded and reconstructed in a BoW representation based on the Variational AutoEncoder. CTM combines Neural-ProdLDA by extending (concatenating) it with contextualized document embeddings from SBERT, which facilitates swift generation of sentence embeddings as opposed to word embeddings (see Fig. 1). On the whole, Bianchi et al. [-@bianchi2020pre] integrated contextualized document embeddings into a neural topic model to produce more coherent topics than previous neural topic models could achieve, while producing competitive results regarding topic diversity. Thus, the combination of a pre-trained document representations and  Bag-of-Words representation to increase the topic coherence appears to successfully overcome the limitation of sole Bag-of-Word (BoW) representations: the consideration of contextual information and word order.


```{r fig4, eval = TRUE, echo = FALSE, out.width = '50%', fig.cap = "High-level architectural schema of a CTM"}
knitr::include_graphics("figures/CTM.png")
```
## Experiments 

**Data**: The train data set consists of publicly available and raw academic papers written by supervisors. Due to the constraint Google Scholar imposes on web scraping, we were forced to manually download papers individually for each supervisor. The test data consists of students' individual thesis proposals, which aside from slight variation and - in contrast to the training data - follow a mostly coherent structure. Thesis proposals were donated by this year's student cohort. Supervision plans are currently used in the CTM as validation data.\n
The unprocessed data sets are made up of individual .pdf files, which were converted into raw text individually and then aggregated into a single .csv file, entailing two columns: the first column carries the file name, the second column holds the document content as a string. However, document contents are cut off after 32,767 characters, the upper limit per cell for .csv files, which we found useful to specify directly within our convertion function. The data sets then undergo preprocessing, i.e. the removal of common and custom-defined stopwords, punctuation, and rare words. 

```{python txt-csv, echo = TRUE, eval = FALSE}

def txt_to_csv(input_dir, output_dir, new_filename): 
  
  files = glob('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + '/*.txt')
  data = [[i, open(i, 'rb').read()] for i in files]
  df = pd.DataFrame(data, columns = ['FileName', 'Content'])
  df['FileName'] = df['FileName'].str.replace('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + '/', '')
  df['Content'] = df['Content'].str.slice(start = 0, stop = 32767) # Upper limit of strings per cell in csv
  df.to_csv(output_dir + '/' + new_filename + '.csv', index = False)
  if not df.empty: 
    print('Succesfully converted txt files in directory ' + os.path.basename('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + ' to single csv file.'))
  else: 
    print('File empty.') 
  return None
```
 
**Software**: We used Google Colab notebooks for tasks that required high computational performance, i.e. GPU usage. The application prototype was developed on the Streamlit framework, using Visual Studio Code on a Windows local machine and PyCharm on a Linux Ubuntu local machine. For Windows, a virtual environment for python using conda from Anaconda was set up to facilitate running the Streamlit app on a local host. For version control, we relied on a shared GitHub repository.

**Evaluation method**: To evaluate model performance of our topic model, we looked at coherence and diversity of topics created. **Normalized Pointwise Mutual Information (NPMI)** is a suitable coherence metric. It uses empirical frequency of words in the original text corpus and measures degree of relatedness of top-10 words to the topic. However, NPMI is computed on the same data the model is trained on and is hence an inherently limited measure of coherence. Therefore, a second coherence measure was employed: external **word embeddings based topic coherence (WETC)**. It calculates average pairwise cosine similarity of the word embeddings of the top-10 words in a topic and computes an average across all topic. It relies on an external vocabulary for comparison, which allows for a more unbiased evaluation.\n
Additionally, the model's topic diversity was evaluated with the **Inverted Rank-Biased Overlap (IRBO)** metric. The method compares two ranked list of topics from the model, i.e. weights the list rank position of same words in a topic, penalizing same words that are higher in a topic list rank. It can take values between 0 and 1, where 0 indicates that all created topics are equal and 1 that they are completely different from one another. 

**Experimental details**: We experimented with setting different numbers of epochs in the hyper-parameter configurations of the CTM (`num_epochs` parameter to 5, 15, 25, 30, 45). As we train on a very small sample, training time was always just a few seconds. We experimented with number of topics to be created as well (`n_components` parameter to 28, 20 and 14 topics) It should be noted that there are currently little options available to further fine-tune and customize the model, as it has been published fairly recently and for research purposes only. 

```{python ctm params, echo = TRUE, eval = FALSE}
ctm = CombinedTM(input_size=len(qt.vocab), bert_input_size=768, num_epochs=25, n_components=14)

ctm.fit(training_dataset)
```

**Results**: We found that the model produces the best outputs with `num_epochs` set to 25 and `n_components` to 14.


**Comment on quantitative results**: Are they what you expected? Better than you expected? Worse than you expected? Why do you think that is? What does this tell you about what you should do next? Including training curves might be useful to discuss whether things are training effectively.

```{r fig5, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Train Loss"}
knitr::include_graphics("figures/trainloss.png")
```


## Analysis 

Your report should include some qualitative evaluation. That is, try to understand your system (how it works, when it succeeds and when it fails) by measuring or inspecting key characteristics or outputs of your model.

- Types of qualitative evaluation include: commenting on selected examples, error analysis, measuring the performance metric for certain subsets of the data, ablation studies, comparing the behaviors of two systems beyond just the performance metric, and visualizing attention distributions or other activation heatmaps.

- The Practical Tips lecture notes has a detailed section on qualitative evaluation -- you may find it useful to reread it.

```{r fig6, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = Topic Distribution across Professors}
knitr::include_graphics("figures/topic-prof.png")
```

## Conclusion(s)

Summarize the main findings of your project, and what you learned. Highlight your achievements, and note the primary limitations of your work. If you like, you can describe avenues for future work.

## Acknowledgments 

As noted repeatedly in the paper, our project is inspired by last semester's students from the class "Python for Data Scientists". We are grateful to their groundwork and the inspiration they provided us.\n 
Throughout the project, we had several guiding meetings with Slava Jankin who, though being a demanding teacher, never ceased to encourage us to dive deeper, experiment more, and give our best. We are also very grateful to Huy Ngoc Dang who amazed us by his willingness to instruct us about different coding challenges, such as visualization or Streamlit implementation, as well as his devotion to the success of our project.\n
We also want to thank Curricular Affairs and all involved members of the school administration who facilitated access to this year's cohorts research proposals (anonymized and voluntary, of course!) for testing purposes.\n
Lastly, our research project would not have been possible without the remarkable work of Bianchi et al. [-@bianchi2020pre]. Their documentation and instructions were of paramount importance to this project and present a top-notch case of collaborative open-source academic work.

