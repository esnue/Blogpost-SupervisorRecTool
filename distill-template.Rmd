---
title: "Thesis Supervisor Recommendation Tool"
description: "Using the Contextualized Topic Model (CTM), we identify academic topics covered by the faculty of the Hertie School in their published papers."
author: 
  - name: Lena Wagner 
    url: 
  - name: Ba Linh Le 
date: "`r Sys.Date()`" 
categories: 
  - Natural Language Processing 
creative_commons: CC BY
repository_url: https://github.com/esnue/ThesisAllocationSystem
output: 
  distill::distill_article: 
    self_contained: false
preview: figures/CTM.png
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
library(reticulate) # For rendering Python code 
```

## Abstract 

Using the Contextualized Topic Model (CTM), we identify academic topics covered by the faculty of the Hertie School in their published papers. The outputs of the model will be available on a web application that aims to support students in their thesis supervisor selection. On the web application, students can get an accessible overview of all potential supervisors' research fields to find the one that can best guide them substantively. The tool is especially recommended for students that have not been able to get familiar with the whole body of the faculty and would like to make sure their supervisor is the best fit for them. The CTM is most successful in identifying coherent topics on texts that entails proper sentences with a semantic structure as opposed to bullet points with very low word count. 
 

## Introduction / Background

If you've happened to come across this website, you're either a student that is taking the NLP class at Hertie and would like to get a great example of a potential semester project or you're a 2nd year student anxious to get started on the MA thesis road. In any case, you've come to the right place.\n

This project was inspired by the recurring challenge in the fall semester, when Hertie students and administrative staff alike have to match suitable and available supervisors to students' master theses. The task is - as you can probably imagine - very laborious. As a Hertie student, you are required to indicate three preferences of supervising professors in decreasing order of rank. But what if you have no idea? You've probably only met 5 professors, but there are 28 options out there! The agony. Worst of all? There is no good overview on the Moodle platform: you're forced to manually and individually download the supervision plan. 

```{r fig1, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: Own work."}
knitr::include_graphics("figures/meme1.jpg")
```

But don't lose hope yet because your seniors are here to come to your aid! We know that the choice of the supervisor matters. It certainly is relevant to the progress and quality of your thesis, potentially even to your work opportunities after graduation! That is why we have developed this tool. \n

The tool provides a lucid and accessible overview over the wide range of fields of academic expertise in the school's faculty body. It expands the work of last semester's student from the class "Python for Data Scientists", who built a simple LDA model and similarity index based on user input.^[Available under (https://github.com/cbsobral/python)] Sounds like a mouthful? Is a mouthful. No worries, everything is going to be explained below. Just stay with us!\n 

However, in case you are familar with these terms and totally know what's going on, here is the **TDLR**: LDA's biggest advantage, Bag-of-Words, turns out to be its deficiency. CTM addresses the lack of contextual information capture by integrating pre-trained BERT sentence embeddings. [@bianchi2020pre] CTM is a fairly new model, but it's proven to be super successful in our case!


## Related Work 

We've been throwing around some fancy terms making us appear real clever, but that is not the goal of this blog post. We want you to understand what's going on! After all, what is a teacher's worth if the students don't get it?\n

The general challenge of topic modeling is the detection of underlying topics in documents and the correct identification of patterns to match documents to topics. Documents describes the text unit of interest such as individual Twitter tweets, mails, papers, reports, etc. So the goal is to be able to identify topics and to create a pattern that is able to match unseen documents to established topics.\n

Let's say you've been tasked with sorting mail. Super tedious work. As you begin to go through each of the letters, you identify categories of letters: bills, advertisement, reminders and notifications of changes. The first pile is done, now on to the next pile we go. However, this pile is somewhat different from the first pile. However, you're clever and confident that you can clearly match the unseen letters to the categories (topics) you've established in the first pile. Get it?\n 

But how does a machine even comprehend text? Easy - by numbers! Every word is turned into a token, i.e. a number. We call that step "encoding". Fancy term there! Now that we've established the foundations, let's get into the nitty-gritty details!\n

```{r fig2, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971"}
knitr::include_graphics("figures/bow.png")
```

Latent Dirichlet Allocation (LDA) is a popular machine learning probabilistic algorithm used for topic modeling. The figure below illustrates how it basically works. It discovers topics and then matches these topics to documents. In the end, you get a table of documents with their proportional share to each topics. Going back to our mailbox example: let's say the first letter (document) is advertisement but there is also a bit of a reminder. Ideally, LDA would return that the document of interest is 60\% advertisement and 39\% reminder, 0.5\% bill and 0.5\% notification of changes. Why 0.5\%? That is how probability works! You can never fully rule out that something happens, you just know that it is veeeeery unlikely to happen.\n

```{r fig3, eval = TRUE, echo = FALSE, out.width = '70%', fig.cap = "Source: https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html"}
knitr::include_graphics("figures/lda.png")
```

But what's machine learning? Standard LDA models are trained once on a training set, i.e. there is one process of identifying and establishing topics. It might be a long process but after the process is done, there is no more training. The model is left untouched, its sole purpose then is to be employed. That is different for deep learning models! They're configured to continuously learn. Every time they get new input, parameters inside the models are set to achieve optimal performance.\n 

But how exactly does LDA find topics? The answer to that very good question is Bag-of-Words (BoW - not to be mistaken with the immensely popular Nintendo Switch game Breath of the Wild (BOTW)). Each document is considered a collection of words, the goal is to identify words distinctive to the document. What then? We get the Term Frequency to get the raw count of words within a document and Inverse Document Frequency to indicate the informative value of words by their frequency in a document compared to their frequency across all other documents. So Term Frequency is interested in what words are used very often and Inverse Document Frequency acts a checks and balance! Words such as "and", "or", "but", etc. appear very often but they do not carry any informative value.

LDA models have successfully been used in the past, such as in the Toronto Paper Matching System. [@charlin2013toronto] The task here was to match submitted papers to reviewers for peer-review journals. The challenge is the amount of both submissions and reviewers. The Toronto Paper Matching System, thus, happens to be a very similar use case to ours! The authors used reviewers' publications to elicit initial expertise scores (topics). This inspired our method! However, we decided not to consider LDA for two reasons: 1) This project is part of a "Natural Language Processing" class at Hertie, focusing on Deep Learning. 2) Bag-of-Words representations are also limited in what information they can extract. In perceiving documents as bag of words, it is unable to comprehend the semantics and context of words. The structure of sentences, e.g. the grammatical role and order of words, does not matter to Bag-of-Words.\n

So how do we capture context? One solution is word2vec, which uses word embeddings. Another fancy term here! You're certainly going to sound very smart by the end of this blog post! Simply plut, word embeddings are learned representation for encoded text (number) where words try to capture as much of the semantic, lexical and context information as possible. In other words, how words are encoded, i.e. turned into numbers, matters a great deal. Word2vec employs two different techniques for word embedding: Continuous Bag of Words (CBoW), predicting a word given its context, and Skip-Gram, predicting the context given a word. It's all probabilities really. The end goal is for words with similar meaning to end up with similar encodings or numeric representations. By doing so, word2vec attempts to integrate the context. However, the catch to word2vec is that it only provides one numeric representation for each word. For example, the word "bank" would erroneously have the same numeric representation in the sentences "We went to the river *bank*."and "I need to go to the *bank* to make a deposit."*. However, a river bank is vastly different from a bank that has money! Thus, in combining different meanings and usages of the same word into a single numeric representation, word2vec inadvertently ends up generating context-independent word embeddings.\n

So what's the latest trend? For once, all the previously mentioned techniques are not entirely discarded. On the contrary, people combine them with other things! More on that on the next section. But to answer that question: it is Pre-trained language models. Pre-trained language models are huge models (and by huge, we mean: really huge models that your and our computer would not be able to handle), that have been trained on millions to billions of documents to learn as much as possible. So instead of developing our own model, we use a pre-trained model and just finetune it on our data, i.e. reconfigure it for our purposes. The rationale is the following: the pre-trained model has an understanding of language much greater of what we could build with our flimsy computers, so we leverage them to achieve higher topic coherence. On our own, we've probably end up with weird and random topics, but we pre-trained language models, everything looks so much better!\n

One such pre-trained language model is the Bidirectional Encoder Representations from Transformers (BERT). BERT generates word embeddings in a bidirectional manner, i.e. from both directions, thereby facilitating muliple numeric representations for the same word, depending on the context of the word in any given sentence. Thus, BERT word embeddings are context-dependent!\n

Congratulations, you've mastered Natural Language Processing! Jokes aside, kudos to you for finishing this section! We hope that you now have an understanding of what's coming next.


## Proposed Method 

CTM is based on the neural topic model Neural-ProdLDA and a contextualized document embedded representation created by Sentence-BERT (SBERT). Neural-ProdLDA trains a neural inference network to transform the Bag of Words (BoW) document representation of a given document into a continuous latent representation via the two parameters $\mu$ and $\sigma^2$ of a Gaussian distribution. Instead of using a multinomial distribution over individual words as is common in standard LDA, Neural-ProdLDA uses the weighted product of experts, leading to stark improvements in topic coherence. The continuous latent representation is then decoded and reconstructed in a BoW representation based on the Variational AutoEncoder. CTM combines Neural-ProdLDA by extending (concatenating) it with contextualized document embeddings from SBERT, which facilitates swift generation of sentence embeddings as opposed to word embeddings (see Fig. 1). On the whole, Bianchi et al. [-@bianchi2020pre] integrated contextualized document embeddings into a neural topic model to produce more coherent topics than previous neural topic models could achieve, while producing competitive results regarding topic diversity. Thus, the combination of a pre-trained document representations and  Bag-of-Words representation to increase the topic coherence appears to successfully overcome the limitation of sole Bag-of-Word (BoW) representations: the consideration of contextual information and word order.


```{r fig4, eval = TRUE, echo = FALSE, out.width = '30%', fig.cap = "High-level architectural schema of a CTM"}
knitr::include_graphics("figures/CTM.png")
```
## Experiments 

**Data**: The train data set consists of publicly available and raw academic papers written by supervisors. Due to the constraint Google Scholar imposes on web scraping, we were forced to manually download papers individually for each supervisor. The test data consists of students' individual thesis proposals, which aside from slight variation and - in contrast to the training data - follow a mostly coherent structure. Thesis proposals were donated by this year's student cohort. Supervision plans are currently used in the CTM as validation data.\n
The unprocessed data sets are made up of individual .pdf files, which were converted into raw text individually and then aggregated into a single .csv file, entailing two columns: the first column carries the file name, the second column holds the document content as a string. However, document contents are cut off after 32,767 characters, the upper limit per cell for .csv files, which we found useful to specify directly within our convertion function. The data sets then undergo preprocessing, i.e. the removal of common and custom-defined stopwords, punctuation, and rare words. 

```{python txt-csv, echo = TRUE, eval = FALSE}

def txt_to_csv(input_dir, output_dir, new_filename): 
  
  files = glob('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + '/*.txt')
  data = [[i, open(i, 'rb').read()] for i in files]
  df = pd.DataFrame(data, columns = ['FileName', 'Content'])
  df['FileName'] = df['FileName'].str.replace('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + '/', '')
  df['Content'] = df['Content'].str.slice(start = 0, stop = 32767) # Upper limit of strings per cell in csv
  df.to_csv(output_dir + '/' + new_filename + '.csv', index = False)
  if not df.empty: 
    print('Succesfully converted txt files in directory ' + os.path.basename('/content/drive/MyDrive/ThesisAllocationSystem/' + input_dir + ' to single csv file.'))
  else: 
    print('File empty.') 
  return None
```
 
**Software**: We used Google Colab notebooks for tasks that required high computational performance, i.e. GPU usage. The application prototype was developed on the Streamlit framework, using Visual Studio Code on a Windows local machine and PyCharm on a Linux Ubuntu local machine. For Windows, a virtual environment for python using conda from Anaconda was set up to facilitate running the Streamlit app on a local host. For version control, we relied on a shared GitHub repository.

**Evaluation method**: To evaluate model performance of our topic model, we looked at coherence and diversity of topics created. **Normalized Pointwise Mutual Information (NPMI)** is a suitable coherence metric. It uses empirical frequency of words in the original text corpus and measures degree of relatedness of top-10 words to the topic. However, NPMI is computed on the same data the model is trained on and is hence an inherently limited measure of coherence. Therefore, a second coherence measure was employed: external **word embeddings based topic coherence (WETC)**. It calculates average pairwise cosine similarity of the word embeddings of the top-10 words in a topic and computes an average across all topic. It relies on an external vocabulary for comparison, which allows for a more unbiased evaluation.\n
Additionally, the model's topic diversity was evaluated with the **Inverted Rank-Biased Overlap (IRBO)** metric. The method compares two ranked list of topics from the model, i.e. weights the list rank position of same words in a topic, penalizing same words that are higher in a topic list rank. It can take values between 0 and 1, where 0 indicates that all created topics are equal and 1 that they are completely different from one another. 

**Experimental details**: We experimented with setting different numbers of epochs in the hyper-parameter configurations of the CTM (`num_epochs` parameter to 5, 15, 25, 30, 45). As we train on a very small sample, training time was always just a few seconds. We experimented with number of topics to be created as well (`n_components` parameter to 28, 20 and 14 topics) It should be noted that there are currently little options available to further fine-tune and customize the model, as it has been published fairly recently and for research purposes only. 

```{python ctm params, echo = TRUE, eval = FALSE}
ctm = CombinedTM(input_size=len(qt.vocab), bert_input_size=768, num_epochs=25, n_components=14)

ctm.fit(training_dataset)
```

**Results**: We found that the model produces the best outputs with `num_epochs` set to 25 and `n_components` to 14.


**Comment on quantitative results**: Are they what you expected? Better than you expected? Worse than you expected? Why do you think that is? What does this tell you about what you should do next? Including training curves might be useful to discuss whether things are training effectively.

```{r fig5, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Train Loss"}
knitr::include_graphics("figures/trainloss.png")
```


## Analysis 

Your report should include some qualitative evaluation. That is, try to understand your system (how it works, when it succeeds and when it fails) by measuring or inspecting key characteristics or outputs of your model.

- Types of qualitative evaluation include: commenting on selected examples, error analysis, measuring the performance metric for certain subsets of the data, ablation studies, comparing the behaviors of two systems beyond just the performance metric, and visualizing attention distributions or other activation heatmaps.

- The Practical Tips lecture notes has a detailed section on qualitative evaluation -- you may find it useful to reread it.

```{r fig6, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Topic Distribution across Professors"}
knitr::include_graphics("figures/topic-prof.png")
```

## Conclusion(s)

A main learning for the authors of this paper was that although algorithms generally and learning algorithms specifically hold enormous potential for a wide range of application fields and are an exciting technology to explore as a social scientist, it should not be underestimated how much human intervention is needed to acquire and prepare data for processing by a Deep Learning model. Our current model architecture involves minimal preprocessing, insofar it does not entirely remove non-relevant information from the training documents, such as web URL links, stylistic page information, etc. One very major drawback to this approach is the remaining interfering remains of RegEx conversion^[RegEx provides regular expression matching operations, see here for more: (https://docs.python.org/3/library/re.html)], namely backslashes followed by "n" to indicate new lines. As a result, we end up with a lot of "noisy" words and vocabulary in our model that begin with "n" and are thus not regarded to be the same to identical words that do not start with a "n", i.e. "neach" and "each". We therefore recommend that future students that might want to continue in our line of work invest more time into building a more sophisticated preprocessing module that is capable to handle these issues.\n
Having said that, it is remarkable to see how easily and swiftly Deep Learning algorithms can be implemented using existing modules and architectures. For instance, our model architecture is based on the blueprint by Bianchi et al. [-@bianchi202pre], that was readily available on the web with detailed instructions and explanations. Thus, the bulk of the work was not actually located in the construction of the model itself, but its preparation, configuration and the visualizations of its outputs.\n
Further steps for the future could include using different training datasets of comparable length to the original study. The initial goal of this project was to develop a recommendation system for students based on their thesis proposals. This was not possible due to quality constraints for this data type, as previously mentioned. Ideally, we would need to standardize research proposals such as that complete sentences and a minimum word count would be required. However, cost-benefit ratio for students in such a scenario would needed to be considered. If the model had indeed been able to produce reasonable results in terms of topic prediction, we would have readily implemented a similar architecture as employed by last semester's students. Consequently, students would have been able to submit their thesis proposal to the recommendation system to get a list of recommended professors that could proven helpful for the choice of the supervisors. Having said that, if such data should become available in the future, it would be interesting to see how the model performs with respect to matching the thesis proposals to professors.\n
Moreover, due to time constraints, we were unable to test different vocabulary length settings. It might, however, be insightful to see whether the evaluation measures indeed worsen as the vocabulary length is increased, given the technical jargon commonly used in academic papers and its specificity to different academic disciplines.

## Acknowledgments 

As noted repeatedly in the paper, our project is inspired by last semester's students from the class "Python for Data Scientists". We are grateful to their groundwork and the inspiration they provided us.\n 
Throughout the project, we had several guiding meetings with Slava Jankin who, though being a demanding teacher, never ceased to encourage us to dive deeper, experiment more, and give our best. We are also very grateful to Huy Ngoc Dang who amazed us by his willingness to instruct us about different coding challenges, such as visualization or Streamlit implementation, as well as his devotion to the success of our project.\n
We also want to thank Curricular Affairs and all involved members of the school administration who facilitated access to this year's cohorts research proposals (anonymized and voluntary, of course!) for testing purposes.\n
Lastly, our research project would not have been possible without the remarkable work of Bianchi et al. [-@bianchi2020pre]. Their documentation and instructions were of paramount importance to this project and present a top-notch case of collaborative open-source academic work.

